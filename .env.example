# =============================================================================
# 内部統制テスト評価AIシステム - 環境変数設定
# =============================================================================
# このファイルを .env にコピーして、実際の値を設定してください。
# cp .env.example .env
#
# 注意: .env ファイルは .gitignore に含まれており、Gitにコミットされません。
# =============================================================================

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose one: AZURE_FOUNDRY, AZURE, GCP, AWS, or LOCAL
LLM_PROVIDER=AZURE_FOUNDRY

# -----------------------------------------------------------------------------
# Azure AI Foundry (Recommended for Azure users)
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AZURE_FOUNDRY to use these settings
AZURE_FOUNDRY_ENDPOINT=https://your-project.region.models.ai.azure.com
AZURE_FOUNDRY_API_KEY=your-foundry-api-key
# Available models (2026年2月最新):
#   === OpenAI GPT シリーズ ===
#   gpt-5.2       - フラッグシップ（企業エージェント・コーディング）
#   gpt-5.2-codex - コード特化
#   gpt-5.1       - 推論機能付き
#   gpt-5         - ロジック・マルチステップ向け
#   gpt-5-nano    - 高速・低コスト（推奨）
#   gpt-5-mini    - 軽量版
#   === Anthropic Claude シリーズ (Microsoft Foundry経由) ===
#   claude-opus-4-6   - Anthropic最高性能（エージェントチーム、1Mトークン）
#   claude-opus-4-5   - 高性能モデル
#   claude-sonnet-4-5 - バランス型
#   claude-haiku-4-5  - 高速・低コスト
AZURE_FOUNDRY_MODEL=gpt-5.2
# Optional: API version (default: 2024-08-01-preview)
# AZURE_FOUNDRY_API_VERSION=2024-08-01-preview

# -----------------------------------------------------------------------------
# Azure OpenAI Service
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AZURE to use these settings
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# -----------------------------------------------------------------------------
# GCP Vertex AI
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=GCP to use these settings
# Authentication: Use GOOGLE_APPLICATION_CREDENTIALS or Application Default Credentials
# GCP_PROJECT_ID=your-gcp-project-id
# GCP_LOCATION=us-central1
# Available models (2026年2月 動作確認済み):
#   gemini-2.5-pro        - 高度な推論・コーディング（動作確認済み）
#   gemini-2.5-flash      - 高速・コスト効率（推奨・動作確認済み）
#   gemini-2.5-flash-lite - 超軽量（動作確認済み）
#   gemini-3-pro-preview  - Gemini 3 Pro（Previewアクセス申請が必要）
#   gemini-3-flash-preview - Gemini 3 Flash（Previewアクセス申請が必要）
# Note: Gemini 2.0は2026/3/31に廃止予定
# Note: asia-northeast1 では一部モデルが利用不可のため us-central1 を推奨
# GCP_MODEL_NAME=gemini-2.5-flash

# -----------------------------------------------------------------------------
# AWS Bedrock
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AWS to use these settings
# Authentication: Use IAM role (Lambda) or access keys
# AWS_REGION=ap-northeast-1
# AWS_ACCESS_KEY_ID=your-access-key-id
# AWS_SECRET_ACCESS_KEY=your-secret-access-key
# Available models (2026年2月 動作確認済み):
#   global.anthropic.claude-opus-4-6-v1           - 最高性能（動作確認済み）
#   global.anthropic.claude-opus-4-5-20251101-v1:0 - 高性能（動作確認済み）
#   jp.anthropic.claude-sonnet-4-5-20250929-v1:0  - バランス型 日本リージョン（動作確認済み）
#   anthropic.claude-3-haiku-20240307-v1:0        - 高速・低コスト
# Note: on-demand には inference profile ID (global.*/jp.*) が必要
# AWS_BEDROCK_MODEL_ID=global.anthropic.claude-opus-4-6-v1

# -----------------------------------------------------------------------------
# Ollama (ローカルLLM)
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=LOCAL to use these settings
# Ollama must be installed and running: https://ollama.ai
# Download model: ollama pull llama3.2:8b
# Available models:
#   llama3.2:8b  - バランス型（推奨）
#   llama3.2:70b - 高精度（要高スペック）
#   phi4:3.8b    - 超軽量
#   mistral:7b   - 軽量高速
#   llava:34b    - Vision対応
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:8b
# OLLAMA_VISION_MODEL=llava:34b

# =============================================================================
# OCR Provider Configuration
# =============================================================================
# Choose one: AZURE, AWS, GCP, TESSERACT, or NONE
# If NONE or not set, falls back to pypdf (text-based PDF only)
OCR_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Azure Document Intelligence
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=AZURE to use these settings
AZURE_DI_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_DI_KEY=your-document-intelligence-key

# -----------------------------------------------------------------------------
# AWS Textract
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=AWS to use these settings
# Uses same AWS credentials as LLM provider
# AWS_TEXTRACT_REGION=ap-northeast-1

# -----------------------------------------------------------------------------
# GCP Document AI
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=GCP to use these settings
# GCP_DOCAI_PROJECT_ID=your-project-id
# GCP_DOCAI_LOCATION=us
# GCP_DOCAI_PROCESSOR_ID=your-processor-id

# -----------------------------------------------------------------------------
# Tesseract OCR (Local/OSS)
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=TESSERACT to use these settings
# TESSERACT_CMD=tesseract
# TESSERACT_LANG=jpn+eng

# =============================================================================
# Performance Tuning - セルフリフレクション設定
# =============================================================================
# 計画レビューの最大修正回数（0でスキップ、1-2で修正回数制限）
MAX_PLAN_REVISIONS=1

# 判断レビューの最大修正回数（0でスキップ、1-2で修正回数制限）
MAX_JUDGMENT_REVISIONS=1

# 計画作成を省略するか（true/false）
# trueにするとLLMによる計画生成をスキップし、デフォルト計画を使用
SKIP_PLAN_CREATION=false

# =============================================================================
# Async Job Processing Configuration (504 Timeout対策)
# =============================================================================
# 非同期ジョブ処理のストレージ/キュー設定
# asyncMode: true の場合に使用されます

# -----------------------------------------------------------------------------
# Job Storage Provider
# -----------------------------------------------------------------------------
# Choose one: AZURE, AWS, GCP, or MEMORY (開発用)
JOB_STORAGE_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Job Queue Provider
# -----------------------------------------------------------------------------
# Choose one: AZURE, AWS, GCP, or MEMORY (開発用)
JOB_QUEUE_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Azure Storage (JOB_STORAGE_PROVIDER=AZURE, JOB_QUEUE_PROVIDER=AZURE)
# -----------------------------------------------------------------------------
# Table Storage + Queue Storage の接続文字列
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=your-storage-account;AccountKey=your-key;EndpointSuffix=core.windows.net

# -----------------------------------------------------------------------------
# AWS DynamoDB + SQS (JOB_STORAGE_PROVIDER=AWS, JOB_QUEUE_PROVIDER=AWS)
# -----------------------------------------------------------------------------
# Uses same AWS credentials as LLM provider
# AWS_DYNAMODB_TABLE_NAME=EvaluationJobs
# AWS_SQS_QUEUE_URL=https://sqs.region.amazonaws.com/account-id/evaluation-jobs

# -----------------------------------------------------------------------------
# GCP Firestore + Cloud Tasks (JOB_STORAGE_PROVIDER=GCP, JOB_QUEUE_PROVIDER=GCP)
# -----------------------------------------------------------------------------
# Uses same GCP credentials as LLM provider
# GCP_FIRESTORE_COLLECTION=evaluation_jobs
# GCP_TASKS_QUEUE_PATH=projects/project-id/locations/region/queues/evaluation-jobs
