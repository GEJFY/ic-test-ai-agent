# =============================================================================
# 内部統制テスト評価AIシステム - 環境変数設定
# =============================================================================
# このファイルを .env にコピーして、実際の値を設定してください。
# cp .env.example .env
#
# 注意: .env ファイルは .gitignore に含まれており、Gitにコミットされません。
# =============================================================================

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose one: AZURE_FOUNDRY, AZURE, GCP, AWS, or LOCAL
LLM_PROVIDER=AZURE_FOUNDRY

# -----------------------------------------------------------------------------
# Azure AI Foundry (Recommended for Azure users)
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AZURE_FOUNDRY to use these settings
AZURE_FOUNDRY_ENDPOINT=https://your-project.region.models.ai.azure.com
AZURE_FOUNDRY_API_KEY=your-foundry-api-key
# Available models (2026年2月最新):
#   === OpenAI GPT シリーズ ===
#   gpt-5.2       - フラッグシップ（企業エージェント・コーディング）
#   gpt-5.2-codex - コード特化
#   gpt-5.1       - 推論機能付き
#   gpt-5         - ロジック・マルチステップ向け
#   gpt-5-nano    - 高速・低コスト（推奨）
#   gpt-5-mini    - 軽量版
#   === Anthropic Claude シリーズ (Microsoft Foundry経由) ===
#   claude-opus-4-6   - Anthropic最高性能（エージェントチーム、1Mトークン）
#   claude-opus-4-5   - 高性能モデル
#   claude-sonnet-4-5 - バランス型
#   claude-haiku-4-5  - 高速・低コスト
AZURE_FOUNDRY_MODEL=gpt-5.2
# Optional: API version (default: 2024-08-01-preview)
# AZURE_FOUNDRY_API_VERSION=2024-08-01-preview

# -----------------------------------------------------------------------------
# Azure OpenAI Service
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AZURE to use these settings
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# -----------------------------------------------------------------------------
# GCP Vertex AI
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=GCP to use these settings
# Authentication: Use GOOGLE_APPLICATION_CREDENTIALS or Application Default Credentials
# GCP_PROJECT_ID=your-gcp-project-id
# GCP_LOCATION=global
# Available models (2026年2月 動作確認済み):
#   gemini-3-pro-preview  - 最高性能（動作確認済み・globalリージョン必須）
#   gemini-3-flash-preview - 高速・マルチモーダル（globalリージョン必須）
#   gemini-2.5-pro        - 高度な推論・コーディング（動作確認済み）
#   gemini-2.5-flash      - 高速・コスト効率（動作確認済み）
#   gemini-2.5-flash-lite - 超軽量（動作確認済み）
# Note: Gemini 3.x は global リージョンが必須
# Note: Gemini 2.x は us-central1 でも利用可能
# Note: Gemini 2.0は2026/3/31に廃止予定
# GCP_MODEL_NAME=gemini-3-pro-preview

# -----------------------------------------------------------------------------
# AWS Bedrock
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=AWS to use these settings
# Authentication: Use IAM role (Lambda) or access keys
# AWS_REGION=ap-northeast-1
# AWS_ACCESS_KEY_ID=your-access-key-id
# AWS_SECRET_ACCESS_KEY=your-secret-access-key
# Available models (2026年2月 動作確認済み):
#   global.anthropic.claude-opus-4-6-v1           - 最高性能（動作確認済み）
#   global.anthropic.claude-opus-4-5-20251101-v1:0 - 高性能（動作確認済み）
#   jp.anthropic.claude-sonnet-4-5-20250929-v1:0  - バランス型 日本リージョン（動作確認済み）
#   anthropic.claude-3-haiku-20240307-v1:0        - 高速・低コスト
# Note: on-demand には inference profile ID (global.*/jp.*) が必要
# AWS_BEDROCK_MODEL_ID=global.anthropic.claude-opus-4-6-v1

# -----------------------------------------------------------------------------
# Ollama (ローカルLLM)
# -----------------------------------------------------------------------------
# Set LLM_PROVIDER=LOCAL to use these settings
# Ollama must be installed and running: https://ollama.ai
# Download model: ollama pull llama3.2:8b
# Available models:
#   llama3.2:8b  - バランス型（推奨）
#   llama3.2:70b - 高精度（要高スペック）
#   phi4:3.8b    - 超軽量
#   mistral:7b   - 軽量高速
#   llava:34b    - Vision対応
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:8b
# OLLAMA_VISION_MODEL=llava:34b

# =============================================================================
# OCR Provider Configuration
# =============================================================================
# Choose one: AZURE, AWS, GCP, TESSERACT, YOMITOKU, or NONE
# If NONE or not set, falls back to pypdf (text-based PDF only)
#
# 言語別推奨:
#   日本語      → AZURE or YOMITOKU（高精度）
#   英語        → AZURE or AWS
#   タイ語      → TESSERACT（AWS/GCPは非対応）
#   オランダ語  → TESSERACT（AWS/GCPは対応限定）
OCR_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Azure Document Intelligence
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=AZURE to use these settings
AZURE_DI_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_DI_KEY=your-document-intelligence-key

# -----------------------------------------------------------------------------
# AWS Textract
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=AWS to use these settings
# Uses same AWS credentials as LLM provider
# AWS_TEXTRACT_REGION=ap-northeast-1

# -----------------------------------------------------------------------------
# GCP Document AI
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=GCP to use these settings
# GCP_DOCAI_PROJECT_ID=your-project-id
# GCP_DOCAI_LOCATION=us
# GCP_DOCAI_PROCESSOR_ID=your-processor-id

# -----------------------------------------------------------------------------
# Tesseract OCR (Local/OSS)
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=TESSERACT to use these settings
# Docker/Lambda使用時は言語パックを含むDockerイメージを使用
# 対応言語: jpn(日本語), eng(英語), tha(タイ語), nld(オランダ語)
# TESSERACT_CMD=/usr/bin/tesseract
# TESSERACT_LANG=jpn+eng+tha+nld

# -----------------------------------------------------------------------------
# YomiToku-Pro (AWS Marketplace - 日本語特化OCR)
# -----------------------------------------------------------------------------
# Set OCR_PROVIDER=YOMITOKU to use these settings
# AWS Marketplace版はSageMaker Endpoint経由で呼び出し
# 高精度な日本語OCR（手書き文字、複雑なレイアウトに対応）
# YOMITOKU_ENDPOINT_NAME=yomitoku-pro-endpoint

# =============================================================================
# Performance Tuning - セルフリフレクション設定
# =============================================================================
# 計画レビューの最大修正回数（0でスキップ、最大5）
MAX_PLAN_REVISIONS=1

# 判断レビューの最大修正回数（0でスキップ、最大5）
MAX_JUDGMENT_REVISIONS=1

# 計画作成を省略するか（true/false）
# trueにするとLLMによる計画生成をスキップし、デフォルト計画を使用
SKIP_PLAN_CREATION=false

# 評価タイムアウト（秒）。60〜900の範囲で設定可能
# EVALUATION_TIMEOUT_SECONDS=300

# 同時評価数の最大値（1〜50）
# MAX_CONCURRENT_EVALUATIONS=10

# =============================================================================
# Evidence File Processing - 証憑ファイル制限
# =============================================================================
# 証憑ファイルのサイズ制限・件数制限・スクリーニング設定

# 単一ファイルの最大サイズ（MB）。デフォルト: 10、範囲: 1-50
# MAX_EVIDENCE_FILE_SIZE_MB=10

# 1テスト項目あたりの最大ファイル数。デフォルト: 20、範囲: 1-100
# MAX_EVIDENCE_FILE_COUNT=20

# 全ファイル合計の最大サイズ（MB）。デフォルト: 50、範囲: 5-200
# MAX_EVIDENCE_TOTAL_SIZE_MB=50

# 証憑スクリーニング（関連性フィルタ）の有効/無効。デフォルト: true
# 統制記述・テスト手続きとの関連性に基づき無関係ファイルを除外
ENABLE_EVIDENCE_SCREENING=true

# スクリーニングにLLMを使用するか。デフォルト: false
# falseの場合はキーワードマッチのみ（LLMコスト0）
# EVIDENCE_SCREENING_USE_LLM=false

# =============================================================================
# Logging Configuration
# =============================================================================
# LOG_LEVEL: DEBUG / INFO / WARNING / ERROR / CRITICAL
# LOG_FORMAT: standard（テキスト形式）/ json（JSON形式 - 本番推奨）
# LOG_TO_FILE: true / false
# LOG_DIR: ログファイル出力ディレクトリ（デフォルト: logs/）
LOG_LEVEL=INFO
# LOG_FORMAT=json
# LOG_TO_FILE=false

# =============================================================================
# Async Job Processing Configuration (504 Timeout対策)
# =============================================================================
# 非同期ジョブ処理のストレージ/キュー設定
# asyncMode: true の場合に使用されます

# -----------------------------------------------------------------------------
# Job Storage Provider
# -----------------------------------------------------------------------------
# Choose one: AZURE, AWS, GCP, or MEMORY (開発用)
JOB_STORAGE_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Job Queue Provider
# -----------------------------------------------------------------------------
# Choose one: AZURE, AWS, GCP, or MEMORY (開発用)
JOB_QUEUE_PROVIDER=AZURE

# -----------------------------------------------------------------------------
# Azure Storage (JOB_STORAGE_PROVIDER=AZURE, JOB_QUEUE_PROVIDER=AZURE)
# -----------------------------------------------------------------------------
# Table Storage + Queue Storage の接続文字列
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=your-storage-account;AccountKey=your-key;EndpointSuffix=core.windows.net

# -----------------------------------------------------------------------------
# AWS DynamoDB + SQS (JOB_STORAGE_PROVIDER=AWS, JOB_QUEUE_PROVIDER=AWS)
# -----------------------------------------------------------------------------
# Uses same AWS credentials as LLM provider
# AWS_DYNAMODB_TABLE_NAME=EvaluationJobs
# AWS_SQS_QUEUE_URL=https://sqs.region.amazonaws.com/account-id/evaluation-jobs

# -----------------------------------------------------------------------------
# GCP Firestore + Cloud Tasks (JOB_STORAGE_PROVIDER=GCP, JOB_QUEUE_PROVIDER=GCP)
# -----------------------------------------------------------------------------
# Uses same GCP credentials as LLM provider
# GCP_FIRESTORE_COLLECTION=evaluation_jobs
# GCP_TASKS_QUEUE_PATH=projects/project-id/locations/region/queues/evaluation-jobs
